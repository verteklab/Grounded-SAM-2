from flask import Flask, request, jsonify, g
import logging
from logging.handlers import RotatingFileHandler
import psutil  # ç³»ç»Ÿç›‘æ§
import torch   # GPUç›‘æ§
import os      # è¿›ç¨‹ä¿¡æ¯
import time    # æ—¶é—´æˆ³
import uuid    # è¯·æ±‚IDç”Ÿæˆ
import threading  # çº¿ç¨‹ä¿¡æ¯
import traceback  # å¼‚å¸¸è¿½è¸ª
from datetime import datetime
from pathlib import Path
from model_manager import model_manager

# åˆ›å»ºlogsç›®å½•
logs_dir = Path(__file__).parent / "logs"
logs_dir.mkdir(exist_ok=True)

# é…ç½®è¯¦ç»†çš„æ—¥å¿—ç³»ç»Ÿ
# ä½¿ç”¨RotatingFileHandleræ”¯æŒæ—¥å¿—è½®è½¬ï¼ˆæ¯ä¸ªæ–‡ä»¶10MBï¼Œä¿ç•™5ä¸ªå¤‡ä»½ï¼‰
file_handler = RotatingFileHandler(
    logs_dir / "app.log",
    maxBytes=10*1024*1024,  # 10MB
    backupCount=5,
    encoding='utf-8'
)
file_handler.setLevel(logging.INFO)

console_handler = logging.StreamHandler()
console_handler.setLevel(logging.INFO)

# ç»Ÿä¸€çš„æ—¥å¿—æ ¼å¼
formatter = logging.Formatter(
    '%(asctime)s | %(levelname)-8s | PID:%(process)d | TID:%(thread)d | %(name)s:%(lineno)d | %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
file_handler.setFormatter(formatter)
console_handler.setFormatter(formatter)

# é…ç½®æ ¹æ—¥å¿—è®°å½•å™¨
root_logger = logging.getLogger()
root_logger.setLevel(logging.INFO)
root_logger.addHandler(file_handler)
root_logger.addHandler(console_handler)

# è·å–åº”ç”¨æ—¥å¿—è®°å½•å™¨
logger = logging.getLogger(__name__)

app = Flask(__name__)

# è¯·æ±‚ç»Ÿè®¡
_request_stats = {
    'total_requests': 0,
    'success_requests': 0,
    'error_requests': 0,
    'lock': threading.Lock()
}

def load_models_on_startup():
    """åœ¨å¯åŠ¨æ—¶åŠ è½½æ¨¡å‹"""
    logger.info("ğŸ”„ å¯åŠ¨æ—¶åŠ è½½æ¨¡å‹...")
    try:
        model_manager.load_models()
        logger.info("âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼ŒæœåŠ¡å°±ç»ª")
    except Exception as e:
        logger.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}", exc_info=True)
        raise

@app.before_request
def before_request():
    """è¯·æ±‚å‰å¤„ç†ï¼šç”Ÿæˆè¯·æ±‚IDå¹¶è®°å½•è¯·æ±‚ä¿¡æ¯"""
    # ç”Ÿæˆå”¯ä¸€è¯·æ±‚ID
    g.request_id = str(uuid.uuid4())[:8]
    g.start_time = time.time()
    g.thread_id = threading.current_thread().ident
    
    # è®°å½•è¯·æ±‚ä¿¡æ¯
    logger.info(
        f"[{g.request_id}] ğŸ“¥ æ”¶åˆ°è¯·æ±‚ | "
        f"Method={request.method} | "
        f"Path={request.path} | "
        f"Remote={request.remote_addr} | "
        f"PID={os.getpid()} | "
        f"TID={g.thread_id} | "
        f"User-Agent={request.headers.get('User-Agent', 'N/A')[:50]}"
    )
    
    # æ›´æ–°ç»Ÿè®¡
    with _request_stats['lock']:
        _request_stats['total_requests'] += 1

@app.after_request
def after_request(response):
    """è¯·æ±‚åå¤„ç†ï¼šè®°å½•å“åº”ä¿¡æ¯å’Œè€—æ—¶"""
    # è®¡ç®—å¤„ç†æ—¶é—´
    duration = time.time() - g.start_time
    
    # è·å–å“åº”å¤§å°
    response_size = len(response.get_data()) if hasattr(response, 'get_data') else 0
    
    # è®°å½•å“åº”ä¿¡æ¯
    log_level = logging.INFO if response.status_code < 400 else logging.WARNING
    logger.log(
        log_level,
        f"[{g.request_id}] ğŸ“¤ å“åº”å®Œæˆ | "
        f"Status={response.status_code} | "
        f"Duration={duration:.3f}s | "
        f"Size={response_size} bytes | "
        f"PID={os.getpid()} | "
        f"TID={g.thread_id}"
    )
    
    # æ›´æ–°ç»Ÿè®¡
    with _request_stats['lock']:
        if response.status_code < 400:
            _request_stats['success_requests'] += 1
        else:
            _request_stats['error_requests'] += 1
    
    # æ·»åŠ è¯·æ±‚IDåˆ°å“åº”å¤´ï¼ˆä¾¿äºè¿½è¸ªï¼‰
    response.headers['X-Request-ID'] = g.request_id
    response.headers['X-Process-ID'] = str(os.getpid())
    response.headers['X-Thread-ID'] = str(g.thread_id)
    
    return response

@app.route('/health', methods=['GET'])
def health_check():
    """å¥åº·æ£€æŸ¥æ¥å£ - å¢å¼ºç‰ˆ"""
    try:
        # æ£€æŸ¥æ¨¡å‹çŠ¶æ€
        model_status = model_manager.models_loaded
        
        # æ£€æŸ¥GPUçŠ¶æ€
        gpu_available = torch.cuda.is_available()
        gpu_memory_used = 0
        if gpu_available:
            gpu_memory_used = torch.cuda.memory_allocated() // 1024**2  # MB
        
        # æ£€æŸ¥å†…å­˜ä½¿ç”¨
        memory = psutil.virtual_memory()
        
        return jsonify({
            "status": "healthy" if model_status else "unhealthy",
            "models_loaded": model_status,
            "device": model_manager.device,
            "service": "Grounded-SAM2 Flask Service",
            "pid": os.getpid(),  # å½“å‰è¿›ç¨‹ID
            "gpu": {
                "available": gpu_available,
                "memory_used_mb": gpu_memory_used
            },
            "memory": {
                "percent": memory.percent,
                "used_mb": memory.used // 1024**2
            }
        }), 200 if model_status else 503
        
    except Exception as e:
        logger.error(f"å¥åº·æ£€æŸ¥å¤±è´¥: {e}")
        return jsonify({
            "status": "unhealthy",
            "error": str(e)
        }), 500

@app.route('/semantic-segmentation', methods=['POST'])
def inference():
    """æ¨ç†æ¥å£ - ä»…æ”¯æŒ Base64 è¾“å…¥æ ¼å¼"""
    request_id = getattr(g, 'request_id', 'unknown')
    
    # æ£€æŸ¥æ¨¡å‹çŠ¶æ€
    if not model_manager.models_loaded:
        logger.warning(f"[{request_id}] âš ï¸ æ¨¡å‹æœªåŠ è½½ï¼Œè¿”å›503")
        return jsonify({"error": "æ¨¡å‹æœªåŠ è½½å®Œæˆ", "request_id": request_id}), 503
    
    try:
        # è·å–è¯·æ±‚æ•°æ®ï¼ˆæ”¯æŒ JSON å’Œ form-dataï¼‰
        if request.is_json:
            data = request.json
            base64_str = data.get('image_base64')
            text_prompt = data.get('text_prompt', 'road surface.')
            box_threshold = float(data.get('box_threshold', 0.01))
            text_threshold = float(data.get('text_threshold', 0.25))
            epsilon = float(data.get('epsilon', 1.0))
        else:
            base64_str = request.form.get('image_base64')
            text_prompt = request.form.get('text_prompt', 'road surface.')
            box_threshold = float(request.form.get('box_threshold', 0.01))
            text_threshold = float(request.form.get('text_threshold', 0.25))
            epsilon = float(request.form.get('epsilon', 1.0))
        
        # è®°å½•è¯·æ±‚å‚æ•°
        base64_len = len(base64_str) if base64_str else 0
        logger.info(
            f"[{request_id}] ğŸ“‹ è¯·æ±‚å‚æ•° | "
            f"text_prompt='{text_prompt}' | "
            f"box_threshold={box_threshold} | "
            f"text_threshold={text_threshold} | "
            f"epsilon={epsilon} | "
            f"base64_length={base64_len}"
        )
        
        # éªŒè¯ base64 è¾“å…¥
        if not base64_str:
            logger.warning(f"[{request_id}] âš ï¸ ç¼ºå°‘image_base64å‚æ•°")
            return jsonify({"error": "è¯·æä¾› image_base64 å‚æ•°ï¼ˆBase64 ç¼–ç çš„å›¾åƒæ•°æ®ï¼‰", "request_id": request_id}), 400
        
        # è®°å½•æ¨ç†å¼€å§‹
        inference_start = time.time()
        logger.info(f"[{request_id}] ğŸš€ å¼€å§‹æ¨ç† | PID={os.getpid()} | TID={threading.current_thread().ident}")
        
        # æ‰§è¡Œæ¨ç†
        result = model_manager.inference(
            image_base64=base64_str,
            text_prompt=text_prompt,
            box_threshold=box_threshold,
            text_threshold=text_threshold,
            epsilon=epsilon,
            request_id=request_id  # ä¼ é€’è¯·æ±‚IDç”¨äºæ—¥å¿—è¿½è¸ª
        )
        
        # è®°å½•æ¨ç†å®Œæˆ
        inference_duration = time.time() - inference_start
        result_count = result.get('count', 0)
        logger.info(
            f"[{request_id}] âœ… æ¨ç†å®Œæˆ | "
            f"Duration={inference_duration:.3f}s | "
            f"Detected={result_count} objects | "
            f"Status={result.get('status', 'unknown')}"
        )
        
        # æ·»åŠ è¯·æ±‚IDåˆ°å“åº”
        result['request_id'] = request_id
        result['inference_time'] = round(inference_duration, 3)
        
        return jsonify(result)
        
    except Exception as e:
        logger.error(
            f"[{request_id}] âŒ æ¨ç†è¯·æ±‚å¤„ç†å¤±è´¥ | "
            f"Error={str(e)} | "
            f"PID={os.getpid()} | "
            f"TID={threading.current_thread().ident}",
            exc_info=True
        )
        return jsonify({"error": str(e), "request_id": request_id}), 500

@app.route('/stats', methods=['GET'])
def stats():
    """è¯·æ±‚ç»Ÿè®¡æ¥å£"""
    with _request_stats['lock']:
        stats_copy = _request_stats.copy()
    stats_copy.pop('lock', None)  # ç§»é™¤é”å¯¹è±¡
    return jsonify(stats_copy)

@app.route('/metrics', methods=['GET'])
def metrics():
    """æ€§èƒ½ç›‘æ§æ¥å£ - æä¾›è¯¦ç»†ç³»ç»ŸçŠ¶æ€"""
    try:
        # ç³»ç»Ÿå±‚é¢
        cpu_percent = psutil.cpu_percent(interval=1)
        memory = psutil.virtual_memory()
        disk = psutil.disk_usage('/')
        
        # GPUå±‚é¢
        gpu_info = {}
        if torch.cuda.is_available():
            gpu_info = {
                "gpu_count": torch.cuda.device_count(),
                "gpu_memory_allocated_mb": torch.cuda.memory_allocated() // 1024**2,
                "gpu_memory_reserved_mb": torch.cuda.memory_reserved() // 1024**2,
                "gpu_memory_total_mb": torch.cuda.get_device_properties(0).total_memory // 1024**2,
                "gpu_utilization": torch.cuda.utilization() if hasattr(torch.cuda, 'utilization') else 0
            }
        
        # è¿›ç¨‹å±‚é¢
        process = psutil.Process()
        process_info = {
            "pid": process.pid,
            "threads": process.num_threads(),
            "memory_mb": process.memory_info().rss // 1024**2,
            "cpu_percent": process.cpu_percent()
        }
        
        # æ¨¡å‹å±‚é¢
        model_info = {
            "loaded": model_manager.models_loaded,
            "device": model_manager.device,
            "sam2_predictor": model_manager.sam2_predictor is not None,
            "grounding_model": model_manager.grounding_model is not None
        }
        
        return jsonify({
            "system": {
                "cpu_percent": cpu_percent,
                "memory_percent": memory.percent,
                "memory_used_mb": memory.used // 1024**2,
                "memory_total_mb": memory.total // 1024**2,
                "disk_percent": disk.percent,
                "disk_free_gb": disk.free // 1024**3
            },
            "gpu": gpu_info,
            "process": process_info,
            "model": model_info,
            "timestamp": time.time(),
            "request_stats": {
                "total": _request_stats['total_requests'],
                "success": _request_stats['success_requests'],
                "error": _request_stats['error_requests']
            }
        })
        
    except Exception as e:
        logger.error(f"ç›‘æ§æ•°æ®è·å–å¤±è´¥: {e}", exc_info=True)
        return jsonify({"error": str(e)}), 500

@app.errorhandler(Exception)
def handle_exception(e):
    """å…¨å±€å¼‚å¸¸å¤„ç†å™¨ - è®°å½•æ‰€æœ‰æœªæ•è·çš„å¼‚å¸¸"""
    request_id = getattr(g, 'request_id', 'unknown')
    
    # è®°å½•è¯¦ç»†çš„å¼‚å¸¸ä¿¡æ¯
    logger.error(
        f"[{request_id}] âŒ æœªæ•è·çš„å¼‚å¸¸ | "
        f"Error={str(e)} | "
        f"Type={type(e).__name__} | "
        f"PID={os.getpid()} | "
        f"TID={threading.current_thread().ident}",
        exc_info=True
    )
    
    # è¿”å›é”™è¯¯å“åº”
    return jsonify({
        "error": "æœåŠ¡å™¨å†…éƒ¨é”™è¯¯",
        "error_type": type(e).__name__,
        "error_message": str(e),
        "request_id": request_id
    }), 500

@app.route('/')
def index():
    """é¦–é¡µ"""
    return jsonify({
        "message": "Grounded-SAM2æ¨ç†æœåŠ¡",
        "endpoints": {
            "å¥åº·æ£€æŸ¥": "/health",
            "æ€§èƒ½ç›‘æ§": "/metrics",
            "è¯·æ±‚ç»Ÿè®¡": "/stats",
            "æ¨ç†æ¥å£": "/semantic-segmentation(POST)",
            "è¾“å…¥æ ¼å¼": "Base64 ç¼–ç çš„å›¾åƒæ•°æ®",
            "å‚æ•°": {
                "image_base64": "å¿…éœ€ï¼ŒBase64 ç¼–ç çš„å›¾åƒå­—ç¬¦ä¸²",
                "text_prompt": "å¯é€‰ï¼Œæ–‡æœ¬æç¤ºï¼ˆé»˜è®¤: 'road surface.'ï¼‰",
                "box_threshold": "å¯é€‰ï¼Œæ£€æµ‹æ¡†é˜ˆå€¼ï¼ˆé»˜è®¤: 0.01ï¼‰",
                "text_threshold": "å¯é€‰ï¼Œæ–‡æœ¬åŒ¹é…é˜ˆå€¼ï¼ˆé»˜è®¤: 0.25ï¼‰",
                "epsilon": "å¯é€‰ï¼Œå¤šè¾¹å½¢ç®€åŒ–ç²¾åº¦å‚æ•°ï¼ˆé»˜è®¤: 1.0ï¼‰"
            }
        }
    })

if __name__ == '__main__':
    # ä»…ç”¨äºç›´æ¥è°ƒè¯•ï¼Œç”Ÿäº§ç¯å¢ƒé€šè¿‡Gunicornå¯åŠ¨
    app.run(
        host='0.0.0.0',
        port=6155,
        debug=False,  # ç”Ÿäº§ç¯å¢ƒè®¾ä¸ºFalse
        threaded=True  # æ”¯æŒå¹¶å‘
    )