# ç”Ÿäº§ç¯å¢ƒGunicorné…ç½®
# 
# ç³»ç»Ÿèµ„æºåˆ†æï¼š
# - CPU: 192æ ¸å¿ƒï¼ˆAMD EPYC 7K62ï¼‰ï¼Œå¯ç”¨96æ ¸å¿ƒ
# - GPU: RTX 3090 (24GBæ˜¾å­˜)
# - å®é™…æµ‹é‡ï¼šæ¯ä¸ªworkerå ç”¨çº¦5-6GBæ˜¾å­˜ï¼ˆåŒ…æ‹¬æ¨¡å‹å’Œæ¨ç†ä¸´æ—¶æ˜¾å­˜ï¼‰
# 
# ä¼˜åŒ–ç­–ç•¥ï¼š
# 1. Workersæ•°é‡ï¼šæ ¹æ®å®é™…GPUæ˜¾å­˜å ç”¨è®¡ç®—
#    è®¡ç®—å…¬å¼: workers = (GPUæ˜¾å­˜ - ç³»ç»Ÿé¢„ç•™) / æ¯ä¸ªworkeræ˜¾å­˜å ç”¨
#    RTX 3090 (24GB): (24 - 2) / 5.5 â‰ˆ 4ä¸ªworkersï¼ˆä¿å®ˆï¼‰
#    è€ƒè™‘åˆ°æ¨ç†æ—¶çš„ä¸´æ—¶æ˜¾å­˜å³°å€¼å’Œå†…å­˜ç¢ç‰‡ï¼Œå»ºè®®è®¾ç½®ä¸º3-4ä¸ªworkers
# 
# 2. Threadsæ•°é‡ï¼šç”±äºæ¨¡å‹é”çš„å­˜åœ¨ï¼Œæ¯ä¸ªworkerå†…çš„æ¨ç†æ˜¯ä¸²è¡Œçš„
#    threadsä¸»è¦ç”¨äºï¼šæ¥æ”¶è¯·æ±‚ã€I/Oå¤„ç†ã€ç­‰å¾…æ¨¡å‹é”
#    æ¨èï¼š2-4ä¸ªthreads/workerï¼Œæ—¢èƒ½å¤„ç†å¹¶å‘è¯·æ±‚ï¼Œåˆä¸ä¼šæµªè´¹èµ„æº
# 
# 3. æ€»å¹¶å‘èƒ½åŠ›ï¼šworkers Ã— threads = å®é™…å¹¶å‘å¤„ç†æ•°
#    ä½†å—æ¨¡å‹é”é™åˆ¶ï¼ŒçœŸæ­£çš„å¹¶è¡Œæ¨ç†æ•° = workersæ•°é‡
# 
# 4. æ˜¾å­˜ç®¡ç†ï¼šå·²æ·»åŠ æ˜¾å­˜æ¸…ç†æœºåˆ¶ï¼Œé˜²æ­¢å†…å­˜æ³„æ¼
# 5. GPUè®¾å¤‡é€‰æ‹©ï¼šå¯ä»¥é€šè¿‡CUDA_VISIBLE_DEVICESæˆ–gpu_device_idæŒ‡å®š

# GPUè®¾å¤‡é…ç½®
# æ–¹å¼1: ä½¿ç”¨CUDA_VISIBLE_DEVICESç¯å¢ƒå˜é‡ï¼ˆæ¨èï¼‰
# æ–¹å¼2: ä½¿ç”¨gpu_device_idé…ç½®ï¼ˆåœ¨post_forkä¸­è®¾ç½®ï¼‰
# å¦‚æœç³»ç»Ÿæœ‰å¤šä¸ªGPUï¼Œå¯ä»¥æŒ‡å®šä½¿ç”¨å“ªä¸ªGPU
# ä¾‹å¦‚: gpu_device_id = 0 è¡¨ç¤ºä½¿ç”¨GPU 0, gpu_device_id = 1 è¡¨ç¤ºä½¿ç”¨GPU 1
gpu_device_id = 1  # é»˜è®¤ä½¿ç”¨GPU 0ï¼Œå¯ä»¥ä¿®æ”¹ä¸º0, 1, 2ç­‰

# å·¥ä½œè¿›ç¨‹æ•°ï¼šæ ¹æ®å®é™…GPUæ˜¾å­˜å ç”¨è°ƒæ•´ï¼ˆRTX 3090 24GBï¼‰
# å®é™…æµ‹é‡ï¼šæ¯ä¸ªworkerå ç”¨çº¦5-6GBæ˜¾å­˜
# è®¡ç®—å…¬å¼: (24GB - 2GBç³»ç»Ÿé¢„ç•™) / 5.5GB â‰ˆ 4ä¸ªworkers
# ä¿å®ˆé…ç½®ï¼š3ä¸ªworkersï¼ˆé¢„ç•™7GBæ˜¾å­˜ç¼“å†²ï¼Œæ›´å®‰å…¨ï¼‰
# å¹³è¡¡é…ç½®ï¼š4ä¸ªworkersï¼ˆé¢„ç•™2GBï¼Œæ¨èï¼‰
workers = 6  # RTX 3090å®é™…å ç”¨5-6GB/workerï¼Œæ¨è3-4ä¸ªworkers

# æ¯ä¸ªè¿›ç¨‹çº¿ç¨‹æ•°
# ç”±äºæ¨¡å‹é”çš„å­˜åœ¨ï¼Œæ¯ä¸ªworkerå†…çš„æ¨ç†æ˜¯ä¸²è¡Œçš„
# threadsä¸»è¦ç”¨äºæ¥æ”¶æ–°è¯·æ±‚å’Œå¤„ç†I/Oï¼Œæ¨è2-4ä¸ª
threads = 3  # æ¯ä¸ªworker 3ä¸ªçº¿ç¨‹ï¼Œç”¨äºå¤„ç†è¯·æ±‚é˜Ÿåˆ—å’ŒI/O

# è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆæ¨¡å‹æ¨ç†å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ï¼‰
timeout = 300

# CUDAä¸fork()ä¸å…¼å®¹ï¼Œä¸èƒ½ä½¿ç”¨preload_app
# æ¯ä¸ªworkerè¿›ç¨‹éœ€è¦ç‹¬ç«‹åŠ è½½æ¨¡å‹
# æ³¨æ„ï¼šè¿™ä¼šå¯¼è‡´æ¯ä¸ªè¿›ç¨‹éƒ½å ç”¨GPUå†…å­˜ï¼Œæ€»å†…å­˜ = æ¨¡å‹å¤§å° Ã— workeræ•°é‡
preload_app = False

# ç»‘å®šåœ°å€å’Œç«¯å£
bind = "0.0.0.0:6155"

# å®ˆæŠ¤è¿›ç¨‹æ¨¡å¼ï¼ˆåå°è¿è¡Œï¼‰
daemon = True

# è¿›ç¨‹åç§°
proc_name = "grounded-sam2-server"

# æ—¥å¿—é…ç½®
loglevel = "info"
accesslog = "logs/access.log"
errorlog = "logs/error.log"
access_log_format = '%(h)s %(l)s %(u)s %(t)s "%(r)s" %(s)s %(b)s "%(f)s" "%(a)s" %(D)s'

# é˜²æ­¢å†…å­˜æ³„æ¼ï¼šæ¯ä¸ªè¿›ç¨‹å¤„ç†ä¸€å®šæ•°é‡è¯·æ±‚åè‡ªåŠ¨é‡å¯
# å¯¹äºæ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œå»ºè®®è®¾ç½®è¾ƒä½çš„å€¼ä»¥é˜²æ­¢å†…å­˜æ³„æ¼
# ç”±äºå·²æ·»åŠ æ˜¾å­˜æ¸…ç†æœºåˆ¶ï¼Œå¯ä»¥é€‚å½“æé«˜æ­¤å€¼
max_requests = 200  # é™ä½åˆ°200ï¼Œæ›´é¢‘ç¹åœ°é‡å¯workerä»¥é‡Šæ”¾ç´¯ç§¯çš„å†…å­˜ç¢ç‰‡
max_requests_jitter = 20  # éšæœºæŠ–åŠ¨ï¼Œé¿å…æ‰€æœ‰workeråŒæ—¶é‡å¯

# ä¼˜é›…è¶…æ—¶è®¾ç½®
graceful_timeout = 60

# Workerå¯åŠ¨åå›è°ƒï¼šåœ¨æ¯ä¸ªworkerè¿›ç¨‹ä¸­åŠ è½½æ¨¡å‹
def on_starting(server):
    """ä¸»è¿›ç¨‹å¯åŠ¨æ—¶è°ƒç”¨"""
    import logging
    logger = logging.getLogger(__name__)
    logger.info("ğŸš€ Gunicornä¸»è¿›ç¨‹å¯åŠ¨")

def post_fork(server, worker):
    """æ¯ä¸ªworkerè¿›ç¨‹forkåè°ƒç”¨ - åœ¨è¿™é‡ŒåŠ è½½æ¨¡å‹"""
    import logging
    import os
    import torch
    
    logger = logging.getLogger(__name__)
    logger.info(f"ğŸ”„ Workerè¿›ç¨‹ {os.getpid()} å¯åŠ¨ï¼Œæ­£åœ¨åŠ è½½æ¨¡å‹...")
    
    # é…ç½®GPUè®¾å¤‡
    # å¦‚æœç¯å¢ƒå˜é‡CUDA_VISIBLE_DEVICESå·²è®¾ç½®ï¼Œä¼˜å…ˆä½¿ç”¨ç¯å¢ƒå˜é‡
    # å¦åˆ™ä½¿ç”¨é…ç½®ä¸­çš„gpu_device_id
    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
        # ä»é…ç½®æ–‡ä»¶ä¸­è¯»å–GPUè®¾å¤‡ID
        try:
            # è¯»å–é…ç½®æ–‡ä»¶ä¸­çš„gpu_device_id
            # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦è®¿é—®å½“å‰æ¨¡å—çš„å…¨å±€å˜é‡
            import importlib
            import sys
            # è·å–å½“å‰é…ç½®æ¨¡å—
            current_module = sys.modules[__name__]
            if hasattr(current_module, 'gpu_device_id'):
                gpu_id = current_module.gpu_device_id
                os.environ['CUDA_VISIBLE_DEVICES'] = str(gpu_id)
                logger.info(f"ğŸ“Œ ä»é…ç½®æ–‡ä»¶è¯»å–GPUè®¾å¤‡: {gpu_id} (è®¾ç½®CUDA_VISIBLE_DEVICES={gpu_id})")
            else:
                logger.warning("âš ï¸ é…ç½®æ–‡ä»¶ä¸­æœªæ‰¾åˆ°gpu_device_idï¼Œä½¿ç”¨é»˜è®¤GPU 0")
                os.environ['CUDA_VISIBLE_DEVICES'] = '0'
        except Exception as e:
            logger.warning(f"âš ï¸ æ— æ³•ä»é…ç½®æ–‡ä»¶è¯»å–GPUè®¾å¤‡ï¼Œä½¿ç”¨é»˜è®¤GPU 0: {e}")
            os.environ['CUDA_VISIBLE_DEVICES'] = '0'
    else:
        logger.info(f"ğŸ“Œ ä½¿ç”¨ç¯å¢ƒå˜é‡æŒ‡å®šçš„GPU: CUDA_VISIBLE_DEVICES={os.environ.get('CUDA_VISIBLE_DEVICES')}")
    
    # é…ç½®PyTorchæ˜¾å­˜ç®¡ç†ï¼Œå‡å°‘å†…å­˜ç¢ç‰‡
    # expandable_segments: å…è®¸PyTorchåŠ¨æ€æ‰©å±•å†…å­˜æ®µï¼Œå‡å°‘ç¢ç‰‡
    os.environ.setdefault('PYTORCH_CUDA_ALLOC_CONF', 'expandable_segments:True')
    
    # åœ¨æ¯ä¸ªworkerè¿›ç¨‹ä¸­åŠ è½½æ¨¡å‹
    from model_manager import model_manager
    try:
        model_manager.load_models()
        
        # è®°å½•åˆå§‹æ˜¾å­˜ä½¿ç”¨æƒ…å†µå’ŒGPUä¿¡æ¯
        if torch.cuda.is_available():
            initial_memory = torch.cuda.memory_allocated() / 1024**3  # GB
            gpu_name = torch.cuda.get_device_name(0) if torch.cuda.device_count() > 0 else "Unknown"
            gpu_id = os.environ.get('CUDA_VISIBLE_DEVICES', '0')
            logger.info(f"âœ… Workerè¿›ç¨‹ {os.getpid()} æ¨¡å‹åŠ è½½å®Œæˆ")
            logger.info(f"   GPUè®¾å¤‡: {gpu_id} ({gpu_name})")
            logger.info(f"   åˆå§‹æ˜¾å­˜å ç”¨: {initial_memory:.2f} GB")
        else:
            logger.info(f"âœ… Workerè¿›ç¨‹ {os.getpid()} æ¨¡å‹åŠ è½½å®Œæˆï¼ˆä½¿ç”¨CPUï¼‰")
    except Exception as e:
        logger.error(f"âŒ Workerè¿›ç¨‹ {os.getpid()} æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        raise
