# ç”Ÿäº§ç¯å¢ƒGunicorné…ç½®
# 
# ç³»ç»Ÿèµ„æºåˆ†æï¼š
# - CPU: 192æ ¸å¿ƒï¼ˆAMD EPYC 7K62ï¼‰ï¼Œå¯ç”¨96æ ¸å¿ƒ
# - GPU: RTX 3090 (24GBæ˜¾å­˜)
# - å®é™…æµ‹é‡ï¼šæ¯ä¸ªworkerå ç”¨çº¦5-6GBæ˜¾å­˜ï¼ˆåŒ…æ‹¬æ¨¡å‹å’Œæ¨ç†ä¸´æ—¶æ˜¾å­˜ï¼‰
# 
# ä¼˜åŒ–ç­–ç•¥ï¼š
# 1. Workersæ•°é‡ï¼šæ ¹æ®å®é™…GPUæ˜¾å­˜å ç”¨è®¡ç®—
#    è®¡ç®—å…¬å¼: workers = (GPUæ˜¾å­˜ - ç³»ç»Ÿé¢„ç•™) / æ¯ä¸ªworkeræ˜¾å­˜å ç”¨
#    RTX 3090 (24GB): (24 - 2) / 5.5 â‰ˆ 4ä¸ªworkersï¼ˆä¿å®ˆï¼‰
#    è€ƒè™‘åˆ°æ¨ç†æ—¶çš„ä¸´æ—¶æ˜¾å­˜å³°å€¼å’Œå†…å­˜ç¢ç‰‡ï¼Œå»ºè®®è®¾ç½®ä¸º3-4ä¸ªworkers
# 
# 2. Threadsæ•°é‡ï¼šç”±äºæ¨¡å‹é”çš„å­˜åœ¨ï¼Œæ¯ä¸ªworkerå†…çš„æ¨ç†æ˜¯ä¸²è¡Œçš„
#    threadsä¸»è¦ç”¨äºï¼šæ¥æ”¶è¯·æ±‚ã€I/Oå¤„ç†ã€ç­‰å¾…æ¨¡å‹é”
#    æ¨èï¼š2-4ä¸ªthreads/workerï¼Œæ—¢èƒ½å¤„ç†å¹¶å‘è¯·æ±‚ï¼Œåˆä¸ä¼šæµªè´¹èµ„æº
# 
# 3. æ€»å¹¶å‘èƒ½åŠ›ï¼šworkers Ã— threads = å®é™…å¹¶å‘å¤„ç†æ•°
#    ä½†å—æ¨¡å‹é”é™åˆ¶ï¼ŒçœŸæ­£çš„å¹¶è¡Œæ¨ç†æ•° = workersæ•°é‡
# 
# 4. æ˜¾å­˜ç®¡ç†ï¼šå·²æ·»åŠ æ˜¾å­˜æ¸…ç†æœºåˆ¶ï¼Œé˜²æ­¢å†…å­˜æ³„æ¼
# 5. GPUè®¾å¤‡é€‰æ‹©ï¼šå¯ä»¥é€šè¿‡CUDA_VISIBLE_DEVICESæˆ–gpu_device_idæŒ‡å®š

# GPUè®¾å¤‡é…ç½® - å•GPUæ¨¡å¼
# ä½¿ç”¨CUDA_VISIBLE_DEVICESç¯å¢ƒå˜é‡æŒ‡å®šGPUè®¾å¤‡
import os

# å•GPUé…ç½®
gpu_device_id = int(os.getenv('GPU_DEVICE_ID', '0'))  # æ”¯æŒç¯å¢ƒå˜é‡è¦†ç›–

# å·¥ä½œè¿›ç¨‹æ•°ï¼šæ”¯æŒç¯å¢ƒå˜é‡è¦†ç›–ï¼Œé»˜è®¤3ä¸ªworkers
# è®¡ç®—å…¬å¼: workers = (GPUæ˜¾å­˜ - ç³»ç»Ÿé¢„ç•™) / æ¯ä¸ªworkeræ˜¾å­˜å ç”¨
# RTX 3090 (24GB): (24 - 2) / 5.5 â‰ˆ 4ä¸ªworkersï¼ˆä¿å®ˆï¼‰
# è€ƒè™‘åˆ°æ¨ç†æ—¶çš„ä¸´æ—¶æ˜¾å­˜å³°å€¼å’Œå†…å­˜ç¢ç‰‡ï¼Œå»ºè®®è®¾ç½®ä¸º3-4ä¸ªworkers
workers = int(os.getenv('GUNICORN_WORKERS', '5'))  # æ”¯æŒç¯å¢ƒå˜é‡è¦†ç›–

# æ¯ä¸ªè¿›ç¨‹çº¿ç¨‹æ•°
# ç”±äºæ¨¡å‹é”çš„å­˜åœ¨ï¼Œæ¯ä¸ªworkerå†…çš„æ¨ç†æ˜¯ä¸²è¡Œçš„
# threadsä¸»è¦ç”¨äºæ¥æ”¶æ–°è¯·æ±‚å’Œå¤„ç†I/Oï¼Œæ¨è2-4ä¸ª
# 
# æ³¨æ„ï¼šGunicornä¼šè‡ªåŠ¨åœ¨workersä¹‹é—´è½®è¯¢åˆ†é…è¯·æ±‚ï¼Œå®ç°workerçº§åˆ«çš„è´Ÿè½½å‡è¡¡
# - sync worker (threads=1): æ¯ä¸ªworkerä¸€ä¸ªçº¿ç¨‹ï¼ŒGunicornåœ¨workersä¹‹é—´è½®è¯¢åˆ†é…è¯·æ±‚
# - gthread worker (threads>1): æ¯ä¸ªworkerå¤šä¸ªçº¿ç¨‹ï¼ŒGunicornåœ¨workersä¹‹é—´åˆ†é…è¯·æ±‚ï¼Œworkerå†…éƒ¨ä½¿ç”¨çº¿ç¨‹æ± 
# ä¸¤ç§æ–¹å¼éƒ½ä¼šåœ¨workersä¹‹é—´è‡ªåŠ¨è´Ÿè½½å‡è¡¡ï¼Œæ— éœ€é¢å¤–é…ç½®
threads = int(os.getenv('GUNICORN_THREADS', '3'))  # æ”¯æŒç¯å¢ƒå˜é‡è¦†ç›–

# è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆæ¨¡å‹æ¨ç†å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ï¼‰
timeout = int(os.getenv('GUNICORN_TIMEOUT', '300'))  # æ”¯æŒç¯å¢ƒå˜é‡è¦†ç›–

# CUDAä¸fork()ä¸å…¼å®¹ï¼Œä¸èƒ½ä½¿ç”¨preload_app
# æ¯ä¸ªworkerè¿›ç¨‹éœ€è¦ç‹¬ç«‹åŠ è½½æ¨¡å‹
# æ³¨æ„ï¼šè¿™ä¼šå¯¼è‡´æ¯ä¸ªè¿›ç¨‹éƒ½å ç”¨GPUå†…å­˜ï¼Œæ€»å†…å­˜ = æ¨¡å‹å¤§å° Ã— workeræ•°é‡
preload_app = False

# ç»‘å®šåœ°å€å’Œç«¯å£
bind = os.getenv('GUNICORN_BIND', '0.0.0.0:6155')  # æ”¯æŒç¯å¢ƒå˜é‡è¦†ç›–

# å®ˆæŠ¤è¿›ç¨‹æ¨¡å¼ï¼ˆåå°è¿è¡Œï¼‰
# Dockerä¸­å¿…é¡»è®¾ä¸ºFalseï¼Œå¦åˆ™å®¹å™¨ä¼šç«‹å³é€€å‡º
daemon = os.getenv('GUNICORN_DAEMON', 'False').lower() == 'true'  # æ”¯æŒç¯å¢ƒå˜é‡è¦†ç›–

# è¿›ç¨‹åç§°
proc_name = "grounded-sam2-server"

# æ—¥å¿—é…ç½®
loglevel = os.getenv('GUNICORN_LOGLEVEL', 'info')  # æ”¯æŒç¯å¢ƒå˜é‡è¦†ç›–
accesslog = "logs/access.log"
errorlog = "logs/error.log"
access_log_format = '%(h)s %(l)s %(u)s %(t)s "%(r)s" %(s)s %(b)s "%(f)s" "%(a)s" %(D)s'

# é˜²æ­¢å†…å­˜æ³„æ¼ï¼šæ¯ä¸ªè¿›ç¨‹å¤„ç†ä¸€å®šæ•°é‡è¯·æ±‚åè‡ªåŠ¨é‡å¯
# å¯¹äºæ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œå»ºè®®è®¾ç½®è¾ƒä½çš„å€¼ä»¥é˜²æ­¢å†…å­˜æ³„æ¼
# ç”±äºå·²æ·»åŠ æ˜¾å­˜æ¸…ç†æœºåˆ¶ï¼Œå¯ä»¥é€‚å½“æé«˜æ­¤å€¼
# è®¾ç½®ä¸º0è¡¨ç¤ºä¸é™åˆ¶ï¼ˆä¸è‡ªåŠ¨é‡å¯ï¼‰ï¼Œä½†éœ€è¦ç¡®ä¿æ²¡æœ‰å†…å­˜æ³„æ¼
# å¦‚æœé‡åˆ°å†…å­˜æ³„æ¼é—®é¢˜ï¼Œå¯ä»¥è®¾ç½®ä¸º1000-5000ä¹‹é—´çš„å€¼
max_requests = int(os.getenv('GUNICORN_MAX_REQUESTS', '0'))  # 0è¡¨ç¤ºä¸é™åˆ¶ï¼Œæ”¯æŒç¯å¢ƒå˜é‡è¦†ç›–
max_requests_jitter = int(os.getenv('GUNICORN_MAX_REQUESTS_JITTER', '0'))  # æ”¯æŒç¯å¢ƒå˜é‡è¦†ç›–

# ä¼˜é›…è¶…æ—¶è®¾ç½®
graceful_timeout = int(os.getenv('GUNICORN_GRACEFUL_TIMEOUT', '60'))  # æ”¯æŒç¯å¢ƒå˜é‡è¦†ç›–

# Workerå¯åŠ¨åå›è°ƒï¼šåœ¨æ¯ä¸ªworkerè¿›ç¨‹ä¸­åŠ è½½æ¨¡å‹
def on_starting(server):
    """ä¸»è¿›ç¨‹å¯åŠ¨æ—¶è°ƒç”¨"""
    import logging
    logger = logging.getLogger(__name__)
    logger.info("ğŸš€ Gunicornä¸»è¿›ç¨‹å¯åŠ¨")

def post_fork(server, worker):
    """æ¯ä¸ªworkerè¿›ç¨‹forkåè°ƒç”¨ - åœ¨è¿™é‡ŒåŠ è½½æ¨¡å‹"""
    import logging
    import os
    import torch
    
    logger = logging.getLogger(__name__)
    worker_id = os.getpid()  # ä½¿ç”¨è¿›ç¨‹IDä½œä¸ºworkeræ ‡è¯†
    logger.info(f"ğŸ”„ Workerè¿›ç¨‹ {worker_id} å¯åŠ¨ï¼Œæ­£åœ¨åŠ è½½æ¨¡å‹...")
    
    # é…ç½®GPUè®¾å¤‡ - å•GPUæ¨¡å¼
    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
        # ä»é…ç½®æ–‡ä»¶ä¸­è¯»å–GPUè®¾å¤‡ID
        try:
            # è¯»å–é…ç½®æ–‡ä»¶ä¸­çš„gpu_device_id
            import importlib
            import sys
            current_module = sys.modules[__name__]
            if hasattr(current_module, 'gpu_device_id'):
                gpu_id = current_module.gpu_device_id
                os.environ['CUDA_VISIBLE_DEVICES'] = str(gpu_id)
                logger.info(f"ğŸ“Œ ä»é…ç½®æ–‡ä»¶è¯»å–GPUè®¾å¤‡: {gpu_id}")
            else:
                logger.warning("âš ï¸ é…ç½®æ–‡ä»¶ä¸­æœªæ‰¾åˆ°gpu_device_idï¼Œä½¿ç”¨é»˜è®¤GPU 0")
                os.environ['CUDA_VISIBLE_DEVICES'] = '0'
        except Exception as e:
            logger.warning(f"âš ï¸ æ— æ³•ä»é…ç½®æ–‡ä»¶è¯»å–GPUè®¾å¤‡ï¼Œä½¿ç”¨é»˜è®¤GPU 0: {e}")
            os.environ['CUDA_VISIBLE_DEVICES'] = '0'
    else:
        logger.info(f"ğŸ“Œ ä½¿ç”¨ç¯å¢ƒå˜é‡æŒ‡å®šçš„GPU: CUDA_VISIBLE_DEVICES={os.environ.get('CUDA_VISIBLE_DEVICES')}")
    
    # é…ç½®PyTorchæ˜¾å­˜ç®¡ç†ï¼Œå‡å°‘å†…å­˜ç¢ç‰‡
    # expandable_segments: å…è®¸PyTorchåŠ¨æ€æ‰©å±•å†…å­˜æ®µï¼Œå‡å°‘ç¢ç‰‡
    os.environ.setdefault('PYTORCH_CUDA_ALLOC_CONF', 'expandable_segments:True')
    
    # åœ¨æ¯ä¸ªworkerè¿›ç¨‹ä¸­åŠ è½½æ¨¡å‹
    from model_manager import model_manager
    try:
        model_manager.load_models()
        
        # è®°å½•åˆå§‹æ˜¾å­˜ä½¿ç”¨æƒ…å†µå’ŒGPUä¿¡æ¯
        if torch.cuda.is_available():
            initial_memory = torch.cuda.memory_allocated() / 1024**3  # GB
            gpu_name = torch.cuda.get_device_name(0) if torch.cuda.device_count() > 0 else "Unknown"
            gpu_id = os.environ.get('CUDA_VISIBLE_DEVICES', '0')
            logger.info("")
            logger.info("=" * 60)
            logger.info(f"âœ…âœ…âœ… Workerè¿›ç¨‹ {os.getpid()} æ¨¡å‹åŠ è½½æˆåŠŸï¼âœ…âœ…âœ…")
            logger.info(f"   GPUè®¾å¤‡: {gpu_id} ({gpu_name})")
            logger.info(f"   åˆå§‹æ˜¾å­˜å ç”¨: {initial_memory:.2f} GB")
            logger.info(f"   æ¨¡å‹çŠ¶æ€: SAM2 âœ“ | GroundingDINO âœ“")
            logger.info("=" * 60)
            logger.info("")
        else:
            logger.info("")
            logger.info("=" * 60)
            logger.info(f"âœ…âœ…âœ… Workerè¿›ç¨‹ {os.getpid()} æ¨¡å‹åŠ è½½æˆåŠŸï¼ˆä½¿ç”¨CPUï¼‰âœ…âœ…âœ…")
            logger.info(f"   æ¨¡å‹çŠ¶æ€: SAM2 âœ“ | GroundingDINO âœ“")
            logger.info("=" * 60)
            logger.info("")
        
        # å¯åŠ¨çº¿ç¨‹æ± ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        try:
            from pool_config import get_pool_config
            from thread_pool_manager import get_thread_pool_manager
            
            config = get_pool_config()
            if config.enable_thread_pool:
                thread_pool = get_thread_pool_manager()
                thread_pool.set_model_manager(model_manager)
                thread_pool.start()
                model_manager.enable_thread_pool(thread_pool)
                logger.info(f"âœ… Workerè¿›ç¨‹ {os.getpid()} çº¿ç¨‹æ± å¯åŠ¨å®Œæˆ")
            else:
                logger.info(f"â„¹ï¸  Workerè¿›ç¨‹ {os.getpid()} çº¿ç¨‹æ± æœªå¯ç”¨ï¼ˆä½¿ç”¨ä¼ ç»Ÿé”æ¨¡å¼ï¼‰")
        except Exception as e:
            logger.warning(f"âš ï¸  Workerè¿›ç¨‹ {os.getpid()} çº¿ç¨‹æ± å¯åŠ¨å¤±è´¥ï¼Œå›é€€åˆ°é”æ¨¡å¼: {e}")
            # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œå…è®¸å›é€€åˆ°é”æ¨¡å¼
        
    except Exception as e:
        logger.error(f"âŒ Workerè¿›ç¨‹ {os.getpid()} æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        raise

def worker_exit(server, worker):
    """Workerè¿›ç¨‹é€€å‡ºæ—¶è°ƒç”¨ - ç”¨äºæ¸…ç†èµ„æºå’Œè®°å½•æ—¥å¿—"""
    import logging
    import os
    import torch
    
    logger = logging.getLogger(__name__)
    pid = os.getpid()
    
    # è®°å½•é€€å‡ºä¿¡æ¯
    logger.info("")
    logger.info("=" * 60)
    logger.info(f"ğŸ”„ Workerè¿›ç¨‹ {pid} æ­£åœ¨é€€å‡º...")
    
    # è®°å½•é€€å‡ºæ—¶çš„èµ„æºä½¿ç”¨æƒ…å†µ
    try:
        if torch.cuda.is_available():
            final_memory = torch.cuda.memory_allocated() / 1024**3  # GB
            logger.info(f"   é€€å‡ºæ—¶æ˜¾å­˜å ç”¨: {final_memory:.2f} GB")
        
        # è®°å½•è¿›ç¨‹ç»Ÿè®¡ä¿¡æ¯
        try:
            import psutil
            process = psutil.Process(pid)
            mem_info = process.memory_info()
            logger.info(f"   é€€å‡ºæ—¶å†…å­˜å ç”¨: {mem_info.rss / 1024**2:.1f} MB")
        except:
            pass
    except Exception as e:
        logger.warning(f"   æ— æ³•è·å–é€€å‡ºæ—¶èµ„æºä¿¡æ¯: {e}")
    
    # æ¸…ç†çº¿ç¨‹æ± ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    try:
        from thread_pool_manager import get_thread_pool_manager
        thread_pool = get_thread_pool_manager()
        if thread_pool and thread_pool._running:
            logger.info(f"   æ­£åœ¨å…³é—­çº¿ç¨‹æ± ...")
            thread_pool.shutdown(timeout=10.0)
    except Exception as e:
        logger.warning(f"   çº¿ç¨‹æ± å…³é—­å¤±è´¥: {e}")
    
    # æ¸…ç†GPUæ˜¾å­˜
    try:
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
    except:
        pass
    
    logger.info(f"âœ… Workerè¿›ç¨‹ {pid} é€€å‡ºå®Œæˆ")
    logger.info("=" * 60)
    logger.info("")

def worker_abort(worker):
    """Workerè¿›ç¨‹å¼‚å¸¸é€€å‡ºæ—¶è°ƒç”¨"""
    import logging
    import os
    
    logger = logging.getLogger(__name__)
    pid = os.getpid()
    logger.error("")
    logger.error("=" * 60)
    logger.error(f"âŒâŒâŒ Workerè¿›ç¨‹ {pid} å¼‚å¸¸é€€å‡ºï¼âŒâŒâŒ")
    logger.error("   è¿™å¯èƒ½æ˜¯ç”±äºï¼š")
    logger.error("   1. å†…å­˜ä¸è¶³ï¼ˆOOMï¼‰")
    logger.error("   2. æ˜¾å­˜ä¸è¶³")
    logger.error("   3. æœªæ•è·çš„å¼‚å¸¸")
    logger.error("   4. ç³»ç»Ÿä¿¡å·ï¼ˆSIGKILL/SIGTERMï¼‰")
    logger.error("=" * 60)
    logger.error("")
